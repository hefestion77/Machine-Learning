function [J, grad] = costFunctionReg(theta, X, y, lambda)%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using%   theta as the parameter for regularized logistic regression and the%   gradient of the cost w.r.t. to the parameters. m = length(y); % number of training examplesh = (1 + (exp(-(X*theta)))).^(-1); % the hypothesis function for logistic regressionthetasToReg = theta;thetasToReg(1) = []; % this will skip theta0 in the regularization process with lambdaJ = (((-y)'*log(h)-((1-y)'*(log(1-h))))/m) + ((lambda/(2*m))*(thetasToReg)'*(thetasToReg));grad = ((X'* (h - y)) / m) + [0; ((lambda/m)*thetasToReg)]; % adding a 0 here will allow to perform the addition% of the two matrices.%(we had removed the first value of thetasToReg to skip theta0)end